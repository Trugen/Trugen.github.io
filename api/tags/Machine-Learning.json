{"name":"Machine Learning","slug":"Machine-Learning","count":1,"postlist":[{"title":"多任务学习: MMOE 模型","uid":"5f2e864d1b97aa96abca4e22151653bf","slug":"多任务学习-MMOE-模型","date":"2025-04-21T05:14:21.000Z","updated":"2025-04-21T05:28:24.244Z","comments":true,"path":"api/articles/多任务学习-MMOE-模型.json","keywords":null,"cover":"/img/MMoE.png","text":"MMoE（Multi-gate Mixture of Experts）是一种改进的混合专家（MoE）模型，主要用于多任务学习。它的核心创新是为每个任务引入独立的门控机制（Multi-gate），动态调整不同专家（Expert）网络的权重，从而更灵活地捕捉任务间的共性和差异。...","permalink":"/post/多任务学习-MMOE-模型","photos":[],"count_time":{"symbolsCount":822,"symbolsTime":"1 mins."},"categories":[{"name":"Machine Learning","slug":"Machine-Learning","count":1,"path":"api/categories/Machine-Learning.json"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","count":1,"path":"api/tags/Machine-Learning.json"}],"author":{"name":"Chujun Xiang","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/07/27/pCvHCgU.jpg","link":"/","description":"　金猴奋起千钧棒， <br> 　玉宇澄清万里埃。","socials":{"github":"https://github.com/Trugen","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}]}