{"title":"提示工程简介与方法概述","uid":"c5d49efbe59ab9c6266a30eec8df7a97","slug":"提示工程简介与方法概述","date":"2023-07-27T07:41:55.000Z","updated":"2025-04-03T05:09:34.466Z","comments":true,"path":"api/articles/提示工程简介与方法概述.json","keywords":null,"cover":"/img/提示工程.jpg","content":"<h2 id=\"简介\">简介</h2>\n<p>提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。</p>\n<p><strong>意义</strong> 掌握了提示工程相关技能将有助于用户更好地了解大型语言模型的能力和局限性。研究人员可利用提示工程来提升大语言模型处理复杂任务场景的能力，如问答和算术推理能力。开发人员可通过提示工程设计、研发强大的工程技术，实现和大语言模型或其他生态工具的高效接轨。</p>\n<p><strong>误区</strong> 提示工程<em>不仅仅是关于设计和研发提示词</em>。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性，也可以赋能大语言模型，比如借助专业领域知识和外部工具来增强大语言模型能力。</p>\n<h2 id=\"提示词的基本要素\">提示词的基本要素</h2>\n<p><strong>指令</strong>：想要模型执行的特定任务或指令。</p>\n<p><strong>上下文</strong>：包含外部信息或额外的上下文信息，引导语言模型更好地响应。</p>\n<p><strong>输入数据</strong>：用户输入的内容或问题。</p>\n<p><strong>输出指示</strong>：指定输出的类型或格式。</p>\n<h2 id=\"提示词的通用技巧\">提示词的通用技巧</h2>\n<p><strong>从简入繁</strong>：当有一个涉及许多不同子任务的大任务时，可以尝试将任务分解为更简单的子任务，并随着获得更好的结果而不断构建。这避免了在提示设计过程中一开始就添加过多的复杂性。</p>\n<p><strong>指令化操作</strong>：可以使用命令来指示模型执行各种简单任务，例如“写入”、“分类”、“总结”、“翻译”、“排序”等，从而为各种简单任务设计有效的提示。可以使用一些清晰的分隔符，如“###”，来分隔指令和上下文。举例： <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt：</span><br><span class=\"line\">### 指令 ###</span><br><span class=\"line\">将以下文本翻译成西班牙语：</span><br><span class=\"line\">文本：“hello！”</span><br><span class=\"line\"></span><br><span class=\"line\">Output：</span><br><span class=\"line\">¡Hola!</span><br></pre></td></tr></table></figure></p>\n<p><strong>提高具体性</strong>： 提示越具体和详细，结果就越好。可以通过提供示例、格式、描述性提示等方式来强化输出。此外，还需要考虑 Prompt 长度（因为这个是受限制的），所以需要在这点和细节之间做好取舍。举例： <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">提取以下文本中的地名。</span><br><span class=\"line\">所需格式：</span><br><span class=\"line\">地点：&lt;逗号分隔的公司名称列表&gt;</span><br><span class=\"line\">输入：“虽然这些发展对研究人员来说是令人鼓舞的，但仍有许多谜团。里斯本未知的香帕利莫德中心的神经免疫学家Henrique Veiga-Fernandes说：“我们经常在大脑和我们在周围看到的效果之间有一个黑匣子。”“如果我们想在治疗背景下使用它，我们实际上需要了解机制。””</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">地点：里斯本，香帕利莫德中心</span><br></pre></td></tr></table></figure></p>\n<p><strong>保持精确性</strong>：在提高具体性和采用了指令化等操作之后，很容易导致 Prompt 的目的变得不明确甚至是被混淆，换而言之，就是过多的辅助提示词影响了 LLM 对 Prompt 意图的理解。因此需要保持 Prompt 的直接性和适当的简洁性。</p>\n<p><strong>强调需要做的事</strong>：设计 Prompt 时的另一个常见技巧是避免说不要做什么，而是说要做什么。这鼓励更具体化，并关注导致模型产生良好响应的细节。举例： <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt1:</span><br><span class=\"line\">以下是向客户推荐电影的代理程序。不要询问兴趣。不要询问个人信息。</span><br><span class=\"line\">顾客：请根据我的兴趣推荐电影。</span><br><span class=\"line\">客服：</span><br><span class=\"line\"></span><br><span class=\"line\">Output1:</span><br><span class=\"line\">当然，我可以根据您的兴趣推荐电影。您想看什么类型的电影？您喜欢动作片、喜剧片、爱情片还是其他类型的电影？</span><br><span class=\"line\"></span><br><span class=\"line\">Prompt2:</span><br><span class=\"line\">以下是向客户推荐电影的代理程序。客服负责从全球热门电影中推荐电影。它应该避免询问用户的偏好并避免询问个人信息。如果代理没有电影推荐，它应该回答“抱歉，今天找不到电影推荐。”。</span><br><span class=\"line\">顾客：请根据我的兴趣推荐一部电影。</span><br><span class=\"line\">客服：</span><br><span class=\"line\"></span><br><span class=\"line\">Output2:</span><br><span class=\"line\">抱歉，我没有关于您兴趣的任何信息。不过，这是目前全球热门的电影列表：[电影列表]。希望您能找到喜欢的电影！</span><br></pre></td></tr></table></figure></p>\n<h2 id=\"常见提示技术\">常见提示技术</h2>\n<h3 id=\"zero-shot\">Zero-Shot</h3>\n<p>零样本提示即为：不事先给 LLM 提供样例，而直接将问题给出。示例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">将文本分类为中性、负面或正面。</span><br><span class=\"line\">文本：我认为这次假期还可以。</span><br><span class=\"line\">情感：</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">中性</span><br></pre></td></tr></table></figure>\n<h3 id=\"few-shot\">Few-Shot</h3>\n<p>虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。基于此原因，少样本提示可以作为一种技术，以启用上下文学习 (In-Context Learning) 并引导模型实现更好的性能，示例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses</span><br><span class=\"line\">the word whatpu is:</span><br><span class=\"line\">We were traveling in Africa and we saw these very cute whatpus.</span><br><span class=\"line\">To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses</span><br><span class=\"line\">the word farduddle is:</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">When we won the game, we all started to farduddle in celebration.</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">This is awesome! // Negative</span><br><span class=\"line\">This is bad! // Positive</span><br><span class=\"line\">Wow that movie was rad! // Positive</span><br><span class=\"line\">What a horrible show! //</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">Negative</span><br></pre></td></tr></table></figure>\n<p>标准的少样本提示对许多任务都有效，但仍然不是一种完美的技术，特别是在处理更复杂的推理任务的时候，例子如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">这组数字中的奇数加起来是一个偶数：4、8、9、15、12、2、1。</span><br><span class=\"line\">A：答案是False。</span><br><span class=\"line\">这组数字中的奇数加起来是一个偶数：17、10、19、4、8、12、24。</span><br><span class=\"line\">A：答案是True。</span><br><span class=\"line\">这组数字中的奇数加起来是一个偶数：16、11、14、4、8、13、24。</span><br><span class=\"line\">A：答案是True。</span><br><span class=\"line\">这组数字中的奇数加起来是一个偶数：17、9、10、12、13、4、2。</span><br><span class=\"line\">A：答案是False。</span><br><span class=\"line\">这组数字中的奇数加起来是一个偶数：15、32、5、13、82、7、1。</span><br><span class=\"line\">A：</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">答案是True。</span><br></pre></td></tr></table></figure>\n<p>显然这个答案是不靠谱的，在涉及较为复杂的问题时，可能需要经历多个推理步骤才能得到最终结果，因此我们需要更加合适的激励策略。</p>\n<h3 id=\"chain-of-thought-cot\">Chain-of-Thought (CoT)</h3>\n<p>Chain-of-Thought (CoT) 是一种改进的激励策略，用于提高 LLM 复杂推理任务上的性能，如算术推理、常识推理和符号推理。与 ICL 中简单地使用输入-输出对构造提示不同，CoT 合并了中间推理步骤，可以将最终输出引入提示中。</p>\n<p>对于上面的问题，可以参考 CoT 的思路对 Prompt 进行调整：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">这组数中的奇数加起来是偶数：4、8、9、15、12、2、1。</span><br><span class=\"line\">A：将所有奇数相加（9、15、1）得到25。答案为False。</span><br><span class=\"line\">这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。</span><br><span class=\"line\">A：</span><br><span class=\"line\"></span><br><span class=\"line\">Output:</span><br><span class=\"line\">将所有奇数相加（15、5、13、7、1）得到41。答案为False。</span><br></pre></td></tr></table></figure>\n<p>可以看到 LLM 给出了我们所期望的正确回复。</p>\n<p><strong><em>Let’s think step by step</em></strong></p>\n<p>这是一种特殊的零样本 CoT 方法，举例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt1:</span><br><span class=\"line\">我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？</span><br><span class=\"line\"></span><br><span class=\"line\">Output1:</span><br><span class=\"line\">11个苹果</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">Prompt2：</span><br><span class=\"line\">我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？</span><br><span class=\"line\">让我们逐步思考。</span><br><span class=\"line\"></span><br><span class=\"line\">Output2：</span><br><span class=\"line\">首先，您从10个苹果开始。</span><br><span class=\"line\">您给了邻居和修理工各2个苹果，所以您还剩下6个苹果。</span><br><span class=\"line\">然后您买了5个苹果，所以现在您有11个苹果。</span><br><span class=\"line\">最后，您吃了1个苹果，所以您还剩下10个苹果。</span><br></pre></td></tr></table></figure>\n<h3 id=\"self-consistency\">Self-Consistency</h3>\n<p>自我一致性方法的基本宗旨是通过少样本 CoT <strong>采样多个不同的推理路径</strong>，并使用生成结果选择最一致的答案。这有助于提高CoT提示在涉及算术和常识推理的任务中的性能。示例如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Prompt:</span><br><span class=\"line\">Q：林中有15棵树。林业工人今天将在林中种树。完成后，将有21棵树。林业工人今天种了多少棵树？</span><br><span class=\"line\">A：我们从15棵树开始。后来我们有21棵树。差异必须是他们种树的数量。因此，他们必须种了21-15 = 6棵树。答案是6。</span><br><span class=\"line\">Q：停车场有3辆汽车，又来了2辆汽车，停车场有多少辆汽车？</span><br><span class=\"line\">A：停车场已经有3辆汽车。又来了2辆。现在有3 + 2 = 5辆汽车。答案是5。</span><br><span class=\"line\">Q：Leah有32块巧克力，她的姐姐有42块。如果他们吃了35块，他们总共还剩多少块？</span><br><span class=\"line\">A：Leah有32块巧克力，Leah的姐姐有42块。这意味着最初有32 + 42 = 74块巧克力。已经吃了35块。因此，他们总共还剩74-35 = 39块巧克力。答案是39。</span><br><span class=\"line\">Q：Jason有20个棒棒糖。他给Denny一些棒棒糖。现在Jason只有12个棒棒糖。Jason给Denny多少棒棒糖？</span><br><span class=\"line\">A：Jason有20个棒棒糖。因为他现在只有12个，所以他必须把剩下的给Denny。他给Denny的棒棒糖数量必须是20-12 = 8个棒棒糖。答案是8。</span><br><span class=\"line\">Q：Shawn有五个玩具。圣诞节，他从他的父母那里得到了两个玩具。他现在有多少个玩具？</span><br><span class=\"line\">A：他有5个玩具。他从妈妈那里得到了2个，所以在那之后他有5 + 2 = 7个玩具。然后他从爸爸那里得到了2个，所以总共他有7 + 2 = 9个玩具。答案是9。</span><br><span class=\"line\">Q：服务器房间里有9台计算机。从周一到周四，每天都会安装5台计算机。现在服务器房间里有多少台计算机？</span><br><span class=\"line\">A：从周一到周四有4天。每天都添加了5台计算机。这意味着总共添加了4 * 5 =</span><br><span class=\"line\">20台计算机。一开始有9台计算机，所以现在有9 + 20 = 29台计算机。答案是29。</span><br><span class=\"line\">Q：Michael有58个高尔夫球。星期二，他丢失了23个高尔夫球。星期三，他又丢失了2个。星期三结束时他还剩多少个高尔夫球？</span><br><span class=\"line\">A：Michael最初有58个球。星期二他丢失了23个，所以在那之后他有58-23 = 35个球。星期三他又丢失了2个，所以现在他有35-2 = 33个球。答案是33。</span><br><span class=\"line\">Q：Olivia有23美元。她用每个3美元的价格买了五个百吉饼。她还剩多少钱？</span><br><span class=\"line\">A：她用每个3美元的价格买了5个百吉饼。这意味着她花了5</span><br><span class=\"line\">Q：当我6岁时，我的妹妹是我的一半年龄。现在我70岁了，我的妹妹多大？</span><br><span class=\"line\">A：</span><br><span class=\"line\"></span><br><span class=\"line\">Output1:</span><br><span class=\"line\">当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70-3 = 67岁。答案是67。</span><br><span class=\"line\"></span><br><span class=\"line\">Output2:</span><br><span class=\"line\">当叙述者6岁时，他的妹妹是他年龄的一半，也就是3岁。现在叙述者70岁了，他的妹妹应该是70-3 = 67岁。答案是67。</span><br><span class=\"line\"></span><br><span class=\"line\">Output3:</span><br><span class=\"line\">当我6岁时，我的妹妹是我的一半年龄，也就是3岁。现在我70岁了，所以她是70/2 = 35岁。答案是35。</span><br></pre></td></tr></table></figure>\n<h3 id=\"tree-of-thoughts-tot\">Tree of Thoughts (ToT)</h3>\n<p>对于需要<strong>探索</strong>或<strong>预判战略</strong>的复杂任务来说，传统或简单的提示技巧是不够的。最近有研究者提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。</p>\n<p>可以用下面一张图来说明白这几种 Prompt 技巧的关系与区别： <a href=\"https://imgse.com/i/pCvTyLj\"><img src=\"https://s1.ax1x.com/2023/07/27/pCvTyLj.md.png\" alt=\"pCvTyLj.md.png\"></a></p>\n<h3 id=\"reference\">Reference</h3>\n<p>提示工程指南：https://www.promptingguide.ai/zh</p>\n","feature":true,"text":"提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。本文主要对提示工程的基本特点和主流方法进行了介绍。...","permalink":"/post/提示工程简介与方法概述","photos":[],"count_time":{"symbolsCount":"4.9k","symbolsTime":"4 mins."},"categories":[{"name":"LLM","slug":"LLM","count":1,"path":"api/categories/LLM.json"},{"name":"Prompt Engineering","slug":"LLM/Prompt-Engineering","count":1,"path":"api/categories/LLM/Prompt-Engineering.json"}],"tags":[{"name":"LLM","slug":"LLM","count":1,"path":"api/tags/LLM.json"},{"name":"Prompt Engineering","slug":"Prompt-Engineering","count":1,"path":"api/tags/Prompt-Engineering.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%AE%80%E4%BB%8B\"><span class=\"toc-text\">简介</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0\"><span class=\"toc-text\">提示词的基本要素</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%8F%90%E7%A4%BA%E8%AF%8D%E7%9A%84%E9%80%9A%E7%94%A8%E6%8A%80%E5%B7%A7\"><span class=\"toc-text\">提示词的通用技巧</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%B8%B8%E8%A7%81%E6%8F%90%E7%A4%BA%E6%8A%80%E6%9C%AF\"><span class=\"toc-text\">常见提示技术</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#zero-shot\"><span class=\"toc-text\">Zero-Shot</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#few-shot\"><span class=\"toc-text\">Few-Shot</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#chain-of-thought-cot\"><span class=\"toc-text\">Chain-of-Thought (CoT)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#self-consistency\"><span class=\"toc-text\">Self-Consistency</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#tree-of-thoughts-tot\"><span class=\"toc-text\">Tree of Thoughts (ToT)</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#reference\"><span class=\"toc-text\">Reference</span></a></li></ol></li></ol>","author":{"name":"Chujun Xiang","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/07/27/pCvHCgU.jpg","link":"/","description":"　金猴奋起千钧棒， <br> 　玉宇澄清万里埃。","socials":{"github":"https://github.com/Trugen","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"Bitcoin 白皮书阅读手记","uid":"c8cfae81d0221a7bdc286aee2fff1709","slug":"Bitcoin-白皮书阅读手记","date":"2025-04-03T03:20:24.000Z","updated":"2025-04-03T10:38:52.464Z","comments":true,"path":"api/articles/Bitcoin-白皮书阅读手记.json","keywords":null,"cover":"/img/BTC.jpeg","text":"比特币（Bitcoin）是一种去中心化的数字货币，由化名为中本聪（Satoshi Nakamoto）的个人或团体在2008年提出，并于2009年正式推出。它不依赖任何中央机构（如银行或政府）发行或管理，而是通过区块链技术和密码学原理实现点对点（P2P）的交易验证和价值转移。...","permalink":"/post/Bitcoin-白皮书阅读手记","photos":[],"count_time":{"symbolsCount":"6.7k","symbolsTime":"6 mins."},"categories":[{"name":"BlockChain","slug":"BlockChain","count":1,"path":"api/categories/BlockChain.json"}],"tags":[{"name":"BlockChain","slug":"BlockChain","count":1,"path":"api/tags/BlockChain.json"}],"author":{"name":"Chujun Xiang","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/07/27/pCvHCgU.jpg","link":"/","description":"　金猴奋起千钧棒， <br> 　玉宇澄清万里埃。","socials":{"github":"https://github.com/Trugen","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"Hi there!","uid":"ab58292848b10dce6b1c694b6eb9b247","slug":"Greetings","date":"2023-07-27T02:46:39.000Z","updated":"2025-04-03T08:59:00.558Z","comments":true,"path":"api/articles/Greetings.json","keywords":null,"cover":null,"text":"Are you watching? Welcome to my website, here I’ll update some blogs and pieces ...","permalink":"/post/Greetings","photos":[],"count_time":{"symbolsCount":93,"symbolsTime":"1 mins."},"categories":[],"tags":[{"name":"碎碎念","slug":"碎碎念","count":1,"path":"api/tags/碎碎念.json"}],"author":{"name":"Chujun Xiang","slug":"blog-author","avatar":"https://s1.ax1x.com/2023/07/27/pCvHCgU.jpg","link":"/","description":"　金猴奋起千钧棒， <br> 　玉宇澄清万里埃。","socials":{"github":"https://github.com/Trugen","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}